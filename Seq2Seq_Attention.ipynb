{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation（Seq2Seq, Attention）EN -> JA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Attention(train_X, train_Y, tokenizer_en, tokenizer_ja):\n",
    "    import numpy as np\n",
    "    from keras.models import Model\n",
    "    from keras import regularizers\n",
    "    from keras.layers import Input, Permute, Activation, Embedding, Dense, LSTM, concatenate, dot, BatchNormalization\n",
    "    from keras.optimizers import RMSprop, Adam\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    from keras import backend as K\n",
    "\n",
    "    emb_dim = 256\n",
    "    hid_dim = 256\n",
    "    att_dim = 256\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    en_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "    ja_vocab_size = len(tokenizer_ja.word_index) + 1\n",
    "\n",
    "    seqX_len = len(train_X[0])\n",
    "    seqY_len = len(train_Y[0])\n",
    "    \n",
    "    # Building a model architecture -----------------------------------------------------------------------------------\n",
    "    # encoder\n",
    "    encoder_inputs = Input(shape=(seqX_len,))\n",
    "    encoder_embedded = Embedding(en_vocab_size, emb_dim, mask_zero=True)(encoder_inputs)\n",
    "#     encoder_embedded_BN = BatchNormalization()(encoder_embedded)\n",
    "    encoded_seq, *encoder_states = LSTM(hid_dim, return_sequences=True, return_state=True)(encoder_embedded)\n",
    "\n",
    "    # decoder\n",
    "    decoder_inputs = Input(shape=(seqY_len,))\n",
    "    decoder_embedding = Embedding(ja_vocab_size, emb_dim)\n",
    "    decoder_embedded = decoder_embedding(decoder_inputs)\n",
    "#     decoder_embedded_BN = BatchNormalization()(decoder_embedded)\n",
    "    decoder_lstm = LSTM(hid_dim, return_sequences=True, return_state=True)\n",
    "    decoded_seq, _, _ = decoder_lstm(decoder_embedded, initial_state=encoder_states)\n",
    "\n",
    "    # Attention\n",
    "    score_dense = Dense(hid_dim)\n",
    "    score = score_dense(decoded_seq)                        \n",
    "    score = dot([score, encoded_seq], axes=(2,2)) \n",
    "    attention = Activation('softmax')(score) \n",
    "    context = dot([attention, encoded_seq], axes=(2,1)) \n",
    "    concat = concatenate([context, decoded_seq], axis=2)\n",
    "    attention_dense = Dense(att_dim, activation='tanh')\n",
    "#                             kernel_regularizer=regularizers.l2(weight_decay))\n",
    "    attentional = attention_dense(concat)\n",
    "    output_dense = Dense(ja_vocab_size, activation='softmax')\n",
    "    outputs = output_dense(attentional)\n",
    "    \n",
    "    # training\n",
    "    train_target = np.hstack((train_Y[:, 1:], np.zeros((len(train_Y),1), dtype=np.int32)))\n",
    "    \n",
    "    adam1 = Adam(lr=0.001)\n",
    "    adam2 = Adam(lr=0.0003)\n",
    "    rms1 = RMSprop(lr=0.001)\n",
    "    rms2 = RMSprop(lr=0.0003)\n",
    "    early_stopping = EarlyStopping(patience=0, verbose=1)\n",
    "    \n",
    "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "    model.compile(optimizer=rms1, loss='sparse_categorical_crossentropy')\n",
    "    model.fit([train_X, train_Y], np.expand_dims(train_target, -1), \n",
    "              batch_size=128, epochs=15, verbose=2, validation_split=0.2, callbacks=[early_stopping])\n",
    "    \n",
    "    model.compile(optimizer=rms2, loss='sparse_categorical_crossentropy')\n",
    "    model.fit([train_X, train_Y], np.expand_dims(train_target, -1), \n",
    "              batch_size=128, epochs=10, verbose=2, validation_split=0.2,  callbacks=[early_stopping])\n",
    "    \n",
    "    # Prediction -----------------------------------------------------------------------------------------------------\n",
    "    # encoder\n",
    "    encoder_model = Model(encoder_inputs, [encoded_seq]+encoder_states)\n",
    "\n",
    "    # decoder\n",
    "    decoder_states_inputs = [Input(shape=(hid_dim,)), Input(shape=(hid_dim,))]\n",
    "\n",
    "    decoder_inputs = Input(shape=(1,))\n",
    "    decoder_embedded = decoder_embedding(decoder_inputs)\n",
    "    decoded_seq, *decoder_states = decoder_lstm(decoder_embedded, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoded_seq] + decoder_states)\n",
    "\n",
    "    # Attention\n",
    "    encoded_seq_in, decoded_seq_in = Input(shape=(seqX_len, hid_dim)), Input(shape=(1, hid_dim))\n",
    "    score = score_dense(decoded_seq_in)\n",
    "    score = dot([score, encoded_seq_in], axes=(2,2))\n",
    "    attention = Activation('softmax')(score)\n",
    "    context = dot([attention, encoded_seq_in], axes=(2,1))\n",
    "    concat = concatenate([context, decoded_seq_in], axis=2)\n",
    "    attentional = attention_dense(concat)\n",
    "    attention_outputs = output_dense(attentional)\n",
    "\n",
    "    attention_model = Model([encoded_seq_in, decoded_seq_in], [attention_outputs, attention])\n",
    "    \n",
    "    def decode_sequence(input_seq, bos_eos, max_output_length=1000):\n",
    "        encoded_seq, *states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "        target_seq = np.array(bos_eos[0])  # corresponding index to bos_eos[0]=\"<s>\"\n",
    "        output_seq = bos_eos[0][:]\n",
    "#         attention_seq = np.empty((0,len(input_seq[0])))\n",
    "\n",
    "        while True:\n",
    "            decoded_seq, *states_value = decoder_model.predict([target_seq] + states_value)\n",
    "            output_tokens, attention = attention_model.predict([encoded_seq, decoded_seq])\n",
    "            sampled_token_index = [np.argmax(output_tokens[0, -1, :])]\n",
    "            output_seq += sampled_token_index\n",
    "#             attention_seq = np.append(attention_seq, attention[0], axis=0)\n",
    "\n",
    "            if (sampled_token_index == bos_eos[1] or len(output_seq) > max_output_length):\n",
    "                break\n",
    "\n",
    "            target_seq = np.array(sampled_token_index)\n",
    "\n",
    "        return output_seq\n",
    "    \n",
    "    return decode_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39200 samples, validate on 9800 samples\n",
      "Epoch 1/15\n",
      " - 101s - loss: 2.9512 - val_loss: 2.3274\n",
      "Epoch 2/15\n",
      " - 97s - loss: 2.1430 - val_loss: 2.0275\n",
      "Epoch 3/15\n",
      " - 97s - loss: 1.8982 - val_loss: 1.8367\n",
      "Epoch 4/15\n",
      " - 97s - loss: 1.7110 - val_loss: 1.7062\n",
      "Epoch 5/15\n",
      " - 96s - loss: 1.5639 - val_loss: 1.5958\n",
      "Epoch 6/15\n",
      " - 96s - loss: 1.4429 - val_loss: 1.5098\n",
      "Epoch 7/15\n",
      " - 96s - loss: 1.3397 - val_loss: 1.4272\n",
      "Epoch 8/15\n",
      " - 96s - loss: 1.2489 - val_loss: 1.3750\n",
      "Epoch 9/15\n",
      " - 96s - loss: 1.1679 - val_loss: 1.3236\n",
      "Epoch 10/15\n",
      " - 96s - loss: 1.0955 - val_loss: 1.2900\n",
      "Epoch 11/15\n",
      " - 96s - loss: 1.0285 - val_loss: 1.2659\n",
      "Epoch 12/15\n",
      " - 96s - loss: 0.9680 - val_loss: 1.2385\n",
      "Epoch 13/15\n",
      " - 96s - loss: 0.9128 - val_loss: 1.2282\n",
      "Epoch 14/15\n",
      " - 96s - loss: 0.8634 - val_loss: 1.2130\n",
      "Epoch 15/15\n",
      " - 96s - loss: 0.8172 - val_loss: 1.2111\n",
      "Train on 39200 samples, validate on 9800 samples\n",
      "Epoch 1/10\n",
      " - 97s - loss: 0.7386 - val_loss: 1.1910\n",
      "Epoch 2/10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# Load data＆Tokenization\n",
    "def load_texts(file_path):\n",
    "    tokenizer = Tokenizer(filters=\"\")\n",
    "    whole_texts = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        whole_texts.append(\"<s> \" + line.strip() + \" </s>\")\n",
    "        \n",
    "    tokenizer.fit_on_texts(whole_texts)\n",
    "    \n",
    "    return tokenizer.texts_to_sequences(whole_texts), tokenizer\n",
    "\n",
    "def load_dataset():\n",
    "    train_X, tokenizer_en = load_texts('./data/train.en')\n",
    "    train_Y, tokenizer_ja = load_texts('./data/train.ja')\n",
    "    \n",
    "    train_X = pad_sequences(train_X, padding='post')\n",
    "    train_Y = pad_sequences(train_Y, padding='post')\n",
    "\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(train_X, train_Y, test_size=0.02, random_state=seed)\n",
    "    \n",
    "    return train_X, test_X, train_Y, test_Y, tokenizer_en, tokenizer_ja\n",
    "\n",
    "def compute_bleu(refs, preds):\n",
    "    return np.mean([sentence_bleu(r, p, emulate_multibleu=True) for r, p in zip(refs, preds)])\n",
    "\n",
    "def score():\n",
    "    train_X, test_X, train_Y, test_Y, tokenizer_en, tokenizer_ja = load_dataset()\n",
    "    decode_sequence = Attention(train_X, train_Y, tokenizer_en, tokenizer_ja)\n",
    "\n",
    "    bos_eos = tokenizer_ja.texts_to_sequences([\"<s>\", \"</s>\"])\n",
    "    output = [decode_sequence(test_X[i][np.newaxis,:], bos_eos, 1000) for i in range(len(test_X))]\n",
    "    \n",
    "    detokenizer_ja = dict(map(reversed, tokenizer_ja.word_index.items()))\n",
    "    \n",
    "    preds = [[detokenizer_ja[i] for i in output[n][1:-1]] for n in range(len(output))]\n",
    "    refs = [[detokenizer_ja[i] for i in test_Y[n][1:-(np.count_nonzero(test_Y[n]==0)+1)]] for n in range(len(test_Y))]\n",
    "    refs = [[seq] for seq in refs]\n",
    "    \n",
    "    print(compute_bleu(refs, preds))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
